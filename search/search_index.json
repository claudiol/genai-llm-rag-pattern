{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"deploying/","title":"Deploying the RAG pattern","text":""},{"location":"deploying/#assumptions","title":"Assumptions","text":""},{"location":"deploying/#gpus","title":"GPUs","text":"<p>The current demonstration relies on <code>flash-attention</code> to decrease memory consumption for the LLM models. Today support to this limited to specific Nvidia GPUs which this system can work with. GPUs which are known to be good include:</p> <ul> <li>Nvidia L40S</li> <li>Nvidia A100</li> <li>Nvidia H100/H200</li> </ul> <p>Note: The V100 GPUs are not supported.</p>"},{"location":"deploying/#gpu-pool-management-wip","title":"GPU pool management (WIP)","text":"<p>The pattern today allows GPU pools to be managed for scale-out computing via MCAD and Instascale. It is important to note that this is designed primarily to manage scaling for batch workloads.</p> <p>This works where:</p> <ol> <li>The cluster auto-scaler is enabled (e.g. using the assisted installer into your own tenancy on AWS / GCP)</li> <li>Clusters managed via OpenShift Cluster Manager (e.g. ROSA, ARO and OSD)</li> </ol>"},{"location":"deploying/#manual-setup-steps-on-osd","title":"Manual setup steps on OSD","text":"<p>TBC</p>"},{"location":"dev-scripts/","title":"Development scripts","text":"<p>These scripts are useful for development and automation where the gap has not been completely closed.</p>"},{"location":"dev-scripts/#argo-envsh","title":"<code>argo-env.sh</code>","text":"<p>Two argoCD deployments are created by the validated patterns operator. The depending on your identity and RBAC setup you may not get access with <code>cluster-admin</code> or similar.</p> <p>Running (pre-authenticated with <code>oc</code>) <code>sh argo-env.sh</code> will provide the default admin passwords for each argo instance.</p>"},{"location":"developers/","title":"Developers","text":"<p>stuff</p>"},{"location":"developers/#pre-commit","title":"pre-commit","text":"<p>foo</p>"},{"location":"developers/#documentation-checks","title":"Documentation checks","text":"<p>bar</p>"},{"location":"user-stories/","title":"User stories","text":""},{"location":"user-stories/#motivation","title":"Motivation","text":"<p>The intent of these user stories is to provide a north star for this project.</p>"},{"location":"user-stories/#actors","title":"Actors","text":"<ul> <li> <p>Application Developer:</p> </li> <li> <p>Business application owner:\\   The Business Application Owner is concerned with the business value the LLM delivers.</p> </li> <li> <p>Data engineer:\\   The Data Engineer actor is responsible for accessing, importing, and preparing data that is used by the LLM.</p> </li> <li> <p>Data scientist:</p> </li> <li> <p>Platform engineer:</p> </li> <li> <p>Security architect:</p> </li> <li> <p>Security engineer:</p> </li> </ul>"},{"location":"user-stories/#user-stories","title":"User stories","text":"<ul> <li> <p>As an Application Developer I want to be able to serve (multiple) LLM applications from my platform.</p> </li> <li> <p>As Business Application Owner I want my LLM to be up to date with my systems of record.</p> </li> <li> <p>As a Data Engineer I need to manage the lifecycle of data surrounding an LLM application, including: Ingestion &amp; preparation; building training datasets; Collecting QA data.</p> </li> <li> <p>As a Data Scientist I want to be able to produce an optimised LLM model to support the business use case.</p> </li> <li> <p>As a Data Scientist I want ready access to the latest opensource LLM innovations without getting in trouble with security.</p> </li> <li> <p>As a Platform Engineer I want to provide a platform enabling self-service for LLM development from experiment to production, while maintaining efficient use of the infrastructure (as a service) investment</p> </li> <li> <p>As a Security Architect I want LLM development initiatives to comply with my enterprise-wide security policies and controls.</p> </li> <li> <p>As a Security Architect I want to be able to measure and enforce that all software, models, and data come from trusted sources.</p> </li> <li> <p>As a SOC analyst I want sufficient capabilities to detect, investigate and correct security related events as they apply to the LLM platform and workloads</p> </li> </ul>"}]}